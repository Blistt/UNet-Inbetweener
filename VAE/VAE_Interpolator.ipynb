{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkXuZKkdkcyf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder Class\n",
    "    Values:\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white (1 channel), so that's our default.\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, im_chan=1, output_chan=32, hidden_dim=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = output_chan\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, output_chan * 2, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a encoder block of the VAE, \n",
    "        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation\n",
    "        Parameters:\n",
    "        input_channels: how many channels the input feature representation has\n",
    "        output_channels: how many channels the output feature representation should have\n",
    "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "        stride: the stride of the convolution\n",
    "        final_layer: whether we're on the final layer (affects activation and batchnorm)\n",
    "        '''        \n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Encoder: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "        image: a flattened image tensor with dimension (im_dim)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        encoding = disc_pred.view(len(disc_pred), -1)\n",
    "        # The stddev output is treated as the log of the variance of the normal \n",
    "        # distribution by convention and for numerical stability\n",
    "        return encoding[:, :self.z_dim], encoding[:, self.z_dim:].exp()\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decoder Class\n",
    "    Values:\n",
    "    z_dim: the dimension of the noise vector, a scalar\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white, so that's our default\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(z_dim, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a Decoder block of the VAE, \n",
    "        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation\n",
    "        Parameters:\n",
    "        input_channels: how many channels the input feature representation has\n",
    "        output_channels: how many channels the output feature representation should have\n",
    "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "        stride: the stride of the convolution\n",
    "        final_layer: whether we're on the final layer (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Decoder: Given a noise vector, \n",
    "        returns a generated image.\n",
    "        Parameters:\n",
    "        noise: a noise tensor with dimensions (batch_size, z_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeSWffaHJOG"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    VAE Class\n",
    "    Values:\n",
    "    z_dim: the dimension of the noise vector, a scalar\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white, so that's our default\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encode = Encoder(im_chan, z_dim)\n",
    "        self.decode = Decoder(z_dim, im_chan)\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Decoder: Given a noise vector, \n",
    "        returns a generated image.\n",
    "        Parameters:\n",
    "        images: an image tensor with dimensions (batch_size, im_chan, im_height, im_width)\n",
    "        Returns:\n",
    "        decoding: the autoencoded image\n",
    "        q_dist: the z-distribution of the encoding\n",
    "        '''\n",
    "        q_mean, q_stddev = self.encode(images)\n",
    "        q_dist = Normal(q_mean, q_stddev)\n",
    "        z_sample = q_dist.rsample() # Sample once from each distribution, using the `rsample` notation\n",
    "        decoding = self.decode(z_sample)\n",
    "        return decoding, q_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0INzSqbE1-n"
   },
   "outputs": [],
   "source": [
    "reconstruction_loss = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgOvg9QjDKLm"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.kl import kl_divergence\n",
    "def kl_divergence_loss(q_dist):\n",
    "    return kl_divergence(\n",
    "        q_dist, Normal(torch.zeros_like(q_dist.mean), torch.ones_like(q_dist.stddev))\n",
    "    ).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pre_process(img, binarize_at=0.0, resize_to=(0,0)):\n",
    "    if binarize_at > 0.0:\n",
    "        thresh = int(binarize_at * np.max(img))\n",
    "        # Replace all values in img that are less than the max value with 0\n",
    "        img[img<thresh] = 0\n",
    "        img[img>=thresh] = 255\n",
    "\n",
    "    if resize_to != (0,0):\n",
    "        img = cv2.resize(img, resize_to, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    if binarize_at > 0.0:\n",
    "        thresh = int(binarize_at * np.max(img))\n",
    "        # Replace all values in img that are less than the max value with 0\n",
    "        img[img<thresh] = 0\n",
    "        img[img>=thresh] = 255\n",
    "\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('frame2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "print(img.shape)\n",
    "img = pre_process(img, binarize_at=0.75, resize_to=(512,512))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, binarize_at=0.0, resize_to=(94,94)):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.binarize_at = binarize_at\n",
    "        self.resize_to = resize_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet_path = self.data[idx]\n",
    "        #Extracts all three frames of the triplet\n",
    "        triplet = []\n",
    "        for img_path in os.listdir(triplet_path):\n",
    "            img = cv2.imread(os.path.join(triplet_path, img_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if self.binarize_at > 0.0 or self.resize_to != (0,0):\n",
    "                img = pre_process(img, binarize_at=self.binarize_at, resize_to=self.resize_to)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            triplet.append(img)\n",
    "        return triplet[0], triplet[1], triplet[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "data_dir = '/data/farriaga/atd_12k/Line_Art/train_10k'\n",
    "triplet_paths = [os.path.join(data_dir, p) for p in os.listdir(data_dir)]\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "my_dataset = MyDataset(triplet_paths, transform=transform, binarize_at=0.75, resize_to=(94, 94))\n",
    "train_dataloader = DataLoader(my_dataset, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HVVD8HWcMOfb",
    "outputId": "ef0e379e-d6db-4c37-9cf9-0a49cb6e0c3c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "\n",
    "losses = []\n",
    "device = 'cuda:1'\n",
    "vae = VAE().to(device)\n",
    "vae_opt = torch.optim.Adam(vae.parameters(), lr=0.002)\n",
    "for epoch in range(500):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    time.sleep(0.5)\n",
    "    epoch_loss = 0\n",
    "    for images, _, __ in tqdm(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        vae_opt.zero_grad() # Clear out the gradients\n",
    "        recon_images, encoding = vae(images)\n",
    "        loss = reconstruction_loss(recon_images, images) + kl_divergence_loss(encoding).sum()\n",
    "        loss.backward()\n",
    "        vae_opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Plot the list of losses\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    show_tensor_images(images)\n",
    "    plt.title(\"True\")\n",
    "    plt.subplot(1,2,2)\n",
    "    show_tensor_images(recon_images)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "def block (w, h, k, s):\n",
    "    w = (w - k)//s + 1\n",
    "    h = (h - k)//s + 1\n",
    "    return w, h\n",
    "\n",
    "h = 94\n",
    "w = 94\n",
    "\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "print(w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "s = 2\n",
    "\n",
    "w = 1\n",
    "h = 1\n",
    "\n",
    "\n",
    "def block (w, h, k, s):\n",
    "    w = (w - 1)*s + k\n",
    "    h = (h - 1)*s + k\n",
    "    return w, h\n",
    "\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "print(w, h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the block configuration to use in case I want to maintain the original image aspect/ratio (approx 1:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 158\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "s = 2\n",
    "\n",
    "w = 1\n",
    "h = 3\n",
    "\n",
    "\n",
    "def block (w, h, k, s):\n",
    "    w = (w - 1)*s + k\n",
    "    h = (h - 1)*s + k\n",
    "    return w, h\n",
    "\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "print(w, h)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C2W2-VAE (Optional).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
