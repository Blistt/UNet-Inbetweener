{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkXuZKkdkcyf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder Class\n",
    "    Values:\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white (1 channel), so that's our default.\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, im_chan=1, output_chan=32, hidden_dim=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = output_chan\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, output_chan * 2, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a encoder block of the VAE, \n",
    "        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation\n",
    "        Parameters:\n",
    "        input_channels: how many channels the input feature representation has\n",
    "        output_channels: how many channels the output feature representation should have\n",
    "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "        stride: the stride of the convolution\n",
    "        final_layer: whether we're on the final layer (affects activation and batchnorm)\n",
    "        '''        \n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Encoder: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "        image: a flattened image tensor with dimension (im_dim)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        encoding = disc_pred.view(len(disc_pred), -1)\n",
    "        # The stddev output is treated as the log of the variance of the normal \n",
    "        # distribution by convention and for numerical stability\n",
    "        return encoding[:, :self.z_dim], encoding[:, self.z_dim:].exp()\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decoder Class\n",
    "    Values:\n",
    "    z_dim: the dimension of the noise vector, a scalar\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white, so that's our default\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(z_dim, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim * 2, kernel_size=4, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 2, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a Decoder block of the VAE, \n",
    "        corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation\n",
    "        Parameters:\n",
    "        input_channels: how many channels the input feature representation has\n",
    "        output_channels: how many channels the output feature representation should have\n",
    "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "        stride: the stride of the convolution\n",
    "        final_layer: whether we're on the final layer (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Decoder: Given a noise vector, \n",
    "        returns a generated image.\n",
    "        Parameters:\n",
    "        noise: a noise tensor with dimensions (batch_size, z_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "s = 2\n",
    "\n",
    "w = 1\n",
    "h = 1\n",
    "\n",
    "\n",
    "def block (w, h, k, s):\n",
    "    w = (w - 1)*s + k\n",
    "    h = (h - 1)*s + k\n",
    "    return w, h\n",
    "\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "print(w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "def block (w, h, k, s):\n",
    "    w = (w - k)//s + 1\n",
    "    h = (h - k)//s + 1\n",
    "    return w, h\n",
    "\n",
    "h = 94\n",
    "w = 94\n",
    "\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "print(w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqeSWffaHJOG"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    VAE Class\n",
    "    Values:\n",
    "    z_dim: the dimension of the noise vector, a scalar\n",
    "    im_chan: the number of channels of the output image, a scalar\n",
    "            MNIST is black-and-white, so that's our default\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, z_dim=32, im_chan=1, hidden_dim=64):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encode = Encoder(im_chan, z_dim)\n",
    "        self.decode = Decoder(z_dim, im_chan)\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        Function for completing a forward pass of the Decoder: Given a noise vector, \n",
    "        returns a generated image.\n",
    "        Parameters:\n",
    "        images: an image tensor with dimensions (batch_size, im_chan, im_height, im_width)\n",
    "        Returns:\n",
    "        decoding: the autoencoded image\n",
    "        q_dist: the z-distribution of the encoding\n",
    "        '''\n",
    "        q_mean, q_stddev = self.encode(images)\n",
    "        q_dist = Normal(q_mean, q_stddev)\n",
    "        z_sample = q_dist.rsample() # Sample once from each distribution, using the `rsample` notation\n",
    "        decoding = self.decode(z_sample)\n",
    "        print(decoding.shape)\n",
    "        return decoding, q_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0INzSqbE1-n"
   },
   "outputs": [],
   "source": [
    "reconstruction_loss = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgOvg9QjDKLm"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.kl import kl_divergence\n",
    "def kl_divergence_loss(q_dist):\n",
    "    return kl_divergence(\n",
    "        q_dist, Normal(torch.zeros_like(q_dist.mean), torch.ones_like(q_dist.stddev))\n",
    "    ).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pre_process(img, binarize_at=0.0, resize_to=(0,0)):\n",
    "    if binarize_at > 0.0:\n",
    "        thresh = int(binarize_at * np.max(img))\n",
    "        # Replace all values in img that are less than the max value with 0\n",
    "        img[img<thresh] = 0\n",
    "        img[img>=thresh] = 255\n",
    "\n",
    "    if resize_to != (0,0):\n",
    "        img = cv2.resize(img, resize_to, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    if binarize_at > 0.0:\n",
    "        thresh = int(binarize_at * np.max(img))\n",
    "        # Replace all values in img that are less than the max value with 0\n",
    "        img[img<thresh] = 0\n",
    "        img[img>=thresh] = 255\n",
    "\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 93.5, 93.5, -0.5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('frame2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "print(img.shape)\n",
    "img = pre_process(img, binarize_at=0.75, resize_to=(94,94))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, binarize_at=0.0, resize_to=(94,94)):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.binarize_at = binarize_at\n",
    "        self.resize_to = resize_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet_path = self.data[idx]\n",
    "        #Extracts all three frames of the triplet\n",
    "        triplet = []\n",
    "        for img_path in os.listdir(triplet_path):\n",
    "            img = cv2.imread(os.path.join(triplet_path, img_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if self.binarize_at > 0.0 or self.resize_to != (0,0):\n",
    "                img = pre_process(img, binarize_at=self.binarize_at, resize_to=self.resize_to)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            triplet.append(img)\n",
    "        return triplet[0], triplet[1], triplet[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "data_dir = '/data/farriaga/atd_12k/Line_Art/train_10k'\n",
    "triplet_paths = [os.path.join(data_dir, p) for p in os.listdir(data_dir)]\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "my_dataset = MyDataset(triplet_paths, transform=transform, binarize_at=0.75, resize_to=(94, 94))\n",
    "train_dataloader = DataLoader(my_dataset, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HVVD8HWcMOfb",
    "outputId": "ef0e379e-d6db-4c37-9cf9-0a49cb6e0c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 1, 94, 94])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:07<00:29,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 1, 94, 94])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:14<00:21,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 1, 94, 94])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:22<00:15,  7.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.5\u001b[39m)\n\u001b[1;32m     25\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfor\u001b[39;00m images, _, __ \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m     27\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m     vae_opt\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Clear out the gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m triplet \u001b[39m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(triplet_path):\n\u001b[0;32m---> 20\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(triplet_path, img_path), cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinarize_at \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_to \u001b[39m!=\u001b[39m (\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m):\n\u001b[1;32m     22\u001b[0m         img \u001b[39m=\u001b[39m pre_process(img, binarize_at\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinarize_at, resize_to\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_to)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "\n",
    "losses = []\n",
    "device = 'cuda:1'\n",
    "vae = VAE().to(device)\n",
    "vae_opt = torch.optim.Adam(vae.parameters(), lr=0.002)\n",
    "for epoch in range(500):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    time.sleep(0.5)\n",
    "    epoch_loss = 0\n",
    "    for images, _, __ in tqdm(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        vae_opt.zero_grad() # Clear out the gradients\n",
    "        recon_images, encoding = vae(images)\n",
    "        loss = reconstruction_loss(recon_images, images) + kl_divergence_loss(encoding).sum()\n",
    "        loss.backward()\n",
    "        vae_opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Plot the list of losses\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    show_tensor_images(images)\n",
    "    plt.title(\"True\")\n",
    "    plt.subplot(1,2,2)\n",
    "    show_tensor_images(recon_images)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the block configuration to use in case I want to maintain the original image aspect/ratio (approx 1:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 158\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "s = 2\n",
    "\n",
    "w = 1\n",
    "h = 3\n",
    "\n",
    "\n",
    "def block (w, h, k, s):\n",
    "    w = (w - 1)*s + k\n",
    "    h = (h - 1)*s + k\n",
    "    return w, h\n",
    "\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "w, h = block(w, h, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "print(w, h)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C2W2-VAE (Optional).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
